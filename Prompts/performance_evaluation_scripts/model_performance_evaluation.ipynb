{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, hamming_loss, jaccard_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('input-file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labels(label_str):\n",
    "\n",
    "    if pd.isna(label_str) or label_str.strip() == \"\":\n",
    "        return []\n",
    "    \n",
    "    parts = label_str.split(\",\")\n",
    "    cleaned_labels = []\n",
    "    for part in parts:\n",
    "        cleaned = part.strip().replace(\"_\", \" \")\n",
    "        cleaned_labels.append(cleaned)\n",
    "    \n",
    "    return cleaned_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"pred_labels\"] = data[\"classification\"].apply(parse_labels)\n",
    "data[\"FinalLabels\"] = data[\"FinalLabels\"].apply(parse_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(label):\n",
    "    return label.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_label(label):\n",
    "    if isinstance(label, list) and label:\n",
    "        return label[0]\n",
    "    return label\n",
    "\n",
    "data[\"true_cat\"] = data[\"FinalLabels\"].apply(get_first_label)\n",
    "unique_categories = data[\"true_cat\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {}\n",
    "\n",
    "for cat in unique_categories:\n",
    "   \n",
    "    norm_cat = normalize_label(cat)\n",
    "    actual_count = sum(1 for x in data[\"true_cat\"] if x == cat)\n",
    "    \n",
    "    detected_count = 0\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        \n",
    "        if row[\"true_cat\"] == cat:\n",
    "            preds = row[\"pred_labels\"]\n",
    "\n",
    "            if isinstance(preds, list):\n",
    "                normalized_preds = [normalize_label(p) for p in preds]\n",
    "                if norm_cat in normalized_preds:\n",
    "                    detected_count += 1\n",
    "                    \n",
    "    report[cat] = {\"actual\": actual_count, \"detected\": detected_count}\n",
    "\n",
    "print(\"Report:\")\n",
    "for cat, counts in report.items():\n",
    "    print(f\"{cat}: Actual: {counts['actual']} | Model Detected: {counts['detected']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing string by removing white space and making it lower case\n",
    "def normalize(label):\n",
    "    return label.strip().lower()\n",
    "\n",
    "report_metrics = {} # Dictionary to store all the values\n",
    "unique_categories = data[\"true_cat\"].unique() # storing all unique category\n",
    "\n",
    "\n",
    "for cat in unique_categories:\n",
    "    norm_cat = normalize(cat)\n",
    "    \n",
    "    # actual count in data\n",
    "    actual_count = (data[\"true_cat\"].apply(normalize) == norm_cat).sum()\n",
    "    \n",
    "    true_list = data[\"true_cat\"].apply(normalize).tolist()\n",
    "\n",
    "   \n",
    "   # Normalize predicted data\n",
    "    pred_list = []\n",
    "    for preds in data[\"pred_labels\"]:\n",
    "        if isinstance(preds, list):\n",
    "            norm_preds = [normalize(p) for p in preds]\n",
    "            pred_list.append(norm_preds)\n",
    "        else:\n",
    "            pred_list.append([])\n",
    "\n",
    "    # True Positives counts\n",
    "    tp = 0\n",
    "    for true_label, preds in zip(true_list, pred_list):\n",
    "        if true_label == norm_cat and norm_cat in preds:\n",
    "            tp += 1\n",
    "\n",
    "    # Total Predicted Count\n",
    "    predicted_count = 0\n",
    "    for preds in pred_list:\n",
    "        if norm_cat in preds:\n",
    "            predicted_count += 1\n",
    "    \n",
    "    precision = tp / predicted_count if predicted_count > 0 else 0\n",
    "    recall = tp / actual_count if actual_count > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    report_metrics[cat] = {\n",
    "        \"actual\": actual_count,\n",
    "        \"predicted\": predicted_count,\n",
    "        \"tp\": tp,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "print(\"Evaluation Report:\")\n",
    "for cat, metrics in report_metrics.items():\n",
    "    print(f\"Category: {cat}\")\n",
    "    print(f\"  Actual: {metrics['actual']}\")\n",
    "    print(f\"  Model Predicted: {metrics['predicted']}\")\n",
    "    print(f\"  True Positives: {metrics['tp']}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.2f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.2f}\")\n",
    "    print(f\"  F1 Score: {metrics['f1']:.2f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
